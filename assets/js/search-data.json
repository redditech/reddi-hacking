{
  
    
        "post0": {
            "title": "Messing around with fastai - based on the fastbook content and fastai forum notes",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . Does some magic to setup my environment, here I&#39;m looking to see how hard it was to do this manually. The first thing I&#39;ll do is load up fastai instead of fastbook so I still have all the fastai libraries at hand if I need any core or helper functions . !rm -rf /content/sample_data/birds #cleanup for new run . !pip install -Uqq fastai . from fastai.vision.all import * . Running fastbook.setup_book?? showed that there was a boolean IN_COLLAB that called another function fastbook.setup_colab() to do the setup. Inside setup_collab it sets a global variable gdrive to my current path, and then imports google.collab to use drive to mount my google drive. I can do this myself here. . The snippets below are mostly straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . path = Path.cwd() . path . Path(&#39;/content&#39;) . There is also a nice little function for pulling images from Duck Duck Go instead of having to sign up to Microsoft Azure to get a key to use the Bing API for image search . def search_images_ddg(term, max_images=200): &quot;Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images&quot; assert max_images&lt;1000 url = &#39;https://duckduckgo.com/&#39; res = urlread(url,data={&#39;q&#39;:term}) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;, res) assert searchObj requestUrl = url + &#39;i.js&#39; params = dict(l=&#39;us-en&#39;, o=&#39;json&#39;, q=term, vqd=searchObj.group(1), f=&#39;,,,&#39;, p=&#39;1&#39;, v7exp=&#39;a&#39;) urls,data = set(),{&#39;next&#39;:1} while len(urls)&lt;max_images and &#39;next&#39; in data: try: data = urljson(requestUrl,data=params) urls.update(L(data[&#39;results&#39;]).itemgot(&#39;image&#39;)) requestUrl = url + data[&#39;next&#39;] except (URLError,HTTPError): pass time.sleep(0.2) return L(urls) . This code has a reference to an L object from fastcore, which in documentation on L indicates the intention of it is to replace lists. I should dive deeper into this to understand more. . I originally wrote a Trinidad hummingbird classifier from a previous attempt at the fastai course, in honor my native country of Trinidad and Tobago. This time, since I&#39;m now in Australia, and celebrating my 4th year as a Queenslander living in Brisbane, I wanted to take a look at classifying Brisbane birds. There are actually alot of them, I&#39;ll pick just a few that I actually have remembered seeing so I can do the data cleansing step with a fair level of confidence. I added three types of ducks as well, just to make it a little tougher I think, although I&#39;m not sure if I know the differences myself so we will see how my cleansing activities fare. The names are based on this list. Let&#39;s start with the most famous one, the kookaburra. . kookaburra_bird_images = search_images_ddg(&quot;laughing kookaburra bird&quot;) kookaburra_bird_images[0] . &#39;http://www.glenchilton.com/wp-content/uploads/2014/09/laughing_kookaburra_Ian-Montgomery-birdway-com-au.jpg&#39; . type(kookaburra_bird_images) . fastcore.foundation.L . type(kookaburra_bird_images[0]) . str . There&#39;s a method attrgot() that is used to extract the file names from a column when doing this with Bing in Lesson 2 I read up the documentation on and got further clarification on fastai forums here. Duck Duck Go returns just strings, so there&#39;s no need to extract a column attribute here. . path.ls() . (#3) [Path(&#39;/content/.config&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/sample_data&#39;)] . dest = path/&#39;sample_data/kookaburra_bird.jpg&#39; download_url(kookaburra_bird_images[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . brisbane_bird_types = &quot;kookaburra&quot;,&quot;magpie goose&quot;,&quot;australian white ibis&quot;, &quot;australian pelican&quot;, &quot;pacific black duck&quot;, &quot;plumed whistling duck&quot;, &quot;australian wood duck&quot; path = path/&#39;sample_data/birds&#39; path . Path(&#39;/content/sample_data/birds&#39;) . if not path.exists(): path.mkdir() path . Path(&#39;/content/sample_data/birds&#39;) . path.ls() . (#0) [] . It may take some time to run with the default of 200 images. . for o in brisbane_bird_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_ddg(f&#39;{o} bird&#39;) download_images(dest, urls=results) . Download of http://www.environment.nsw.gov.au/images/nature/white_ibisLg.jpg has failed after 5 retries Fix the download manually: $ mkdir -p /content/sample_data/birds/australian white ibis $ cd /content/sample_data/birds/australian white ibis $ wget -c http://www.environment.nsw.gov.au/images/nature/white_ibisLg.jpg $ tar xf white_ibisLg.jpg And re-run your code once the download is successful . Let&#39;s verify we got images as we expected . path.ls() . (#7) [Path(&#39;/content/sample_data/birds/kookaburra&#39;),Path(&#39;/content/sample_data/birds/magpie goose&#39;),Path(&#39;/content/sample_data/birds/australian pelican&#39;),Path(&#39;/content/sample_data/birds/australian white ibis&#39;),Path(&#39;/content/sample_data/birds/pacific black duck&#39;),Path(&#39;/content/sample_data/birds/plumed whistling duck&#39;),Path(&#39;/content/sample_data/birds/australian wood duck&#39;)] . !ls &quot;sample_data/birds&quot; . &#39;australian pelican&#39; kookaburra &#39;plumed whistling duck&#39; &#39;australian white ibis&#39; &#39;magpie goose&#39; &#39;australian wood duck&#39; &#39;pacific black duck&#39; . get_image_files?? . fns = get_image_files(path) fns . (#1735) [Path(&#39;/content/sample_data/birds/kookaburra/00000173.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000145.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000195.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000044.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000047.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000143.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000101.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000188.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000158.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000074.jpg&#39;)...] . len(fns) . 1735 . Check for the corrupt images . failed = verify_images(fns) failed . (#4) [Path(&#39;/content/sample_data/birds/kookaburra/00000034.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000217.jpg&#39;),Path(&#39;/content/sample_data/birds/australian pelican/00000240.jpg&#39;),Path(&#39;/content/sample_data/birds/australian white ibis/00000119.jpg&#39;)] . Remove corrupted images . failed.map(Path.unlink); . I&#39;m going to bring forward the steps to clean up the data here from the book before cleaning, since I&#39;m pretty sure some of these aren&#39;t going to be bird images so I want to get rid of them. Since we don&#39;t have a classifier yet, we&#39;re going to have to use ImagesCleaner to do this pre-training cleaning step . from fastai.vision.widgets import * . We need the widgets to do cleanup. Reference this forum post . We need to see how to browse the individual directories since I don&#39;t think ImagesCleaner gives that directory selector that the ImageClassifierCleaner module does, so let&#39;s peek at how that works . ImageClassifierCleaner?? . ImagesCleaner?? . path.cwd() . Path(&#39;/content&#39;) . path = Path(&#39;sample_data/birds&#39;) . path.ls() . (#7) [Path(&#39;sample_data/birds/kookaburra&#39;),Path(&#39;sample_data/birds/magpie goose&#39;),Path(&#39;sample_data/birds/australian pelican&#39;),Path(&#39;sample_data/birds/australian white ibis&#39;),Path(&#39;sample_data/birds/pacific black duck&#39;),Path(&#39;sample_data/birds/plumed whistling duck&#39;),Path(&#39;sample_data/birds/australian wood duck&#39;)] . Rerun the next three cells substituting the different categories of kookaburra with magpie goose,australian white ibis, australian pelican, pacific black duck, plumed whistling duck, australian wood duck to do some initial pre-training cleaning of possibly irrelevant images from search engine . fns_to_clean = get_image_files(path/&#39;kookaburra&#39;) fns_to_clean . (#234) [Path(&#39;sample_data/birds/kookaburra/00000173.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000145.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000195.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000044.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000047.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000143.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000101.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000188.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000158.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000074.jpg&#39;)...] . cleaner = ImagesCleaner() cleaner.set_fns(fns_to_clean) cleaner . Now that we&#39;ve marked the ones that should get remove, let&#39;s take them out of the image data before we start training our model . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . path.ls() . (#7) [Path(&#39;sample_data/birds/kookaburra&#39;),Path(&#39;sample_data/birds/magpie goose&#39;),Path(&#39;sample_data/birds/australian pelican&#39;),Path(&#39;sample_data/birds/australian white ibis&#39;),Path(&#39;sample_data/birds/pacific black duck&#39;),Path(&#39;sample_data/birds/plumed whistling duck&#39;),Path(&#39;sample_data/birds/australian wood duck&#39;)] . Load the data into the datablock . birds = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = birds.dataloaders(path) . dls.valid.show_batch(max_n=10, nrows=2) . Squishing large images to fit into the size of the image . birds = birds.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = birds.dataloaders(path) dls.valid.show_batch(max_n=10, nrows=2) . Padding the image borders so they fit . birds = birds.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = birds.dataloaders(path) dls.valid.show_batch(max_n=10, nrows=2) . Randomly resizing images to allow it to learn on specific parts of an image during epoch . birds = birds.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = birds.dataloaders(path) dls.train.show_batch(max_n=10, nrows=2, unique=True) . Starting the data augmentation step here, which does a randomization of all three previous steps. . TODO:Add the individual keywords for image rotation, flipping, perspective warping, brightness changes and contrast changes. . birds = birds.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = birds.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . Putting it all together now to train our model . birds = birds.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = birds.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.692304 | 0.229488 | 0.078035 | 00:46 | . epoch train_loss valid_loss error_rate time . 0 | 0.362119 | 0.178386 | 0.072254 | 00:47 | . 1 | 0.267956 | 0.136552 | 0.046243 | 00:46 | . 2 | 0.220220 | 0.170266 | 0.049133 | 00:47 | . 3 | 0.163554 | 0.166847 | 0.049133 | 00:47 | . Take a look at how well it did . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Let&#39;s see where it went wrong . interp.plot_top_losses(10, nrows=10) . If I missed some categorization pre-training, now I can use the code from the book to fix these mistakes here . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . I can now re-run the learner with the fixed dataset and see if that helps improve anything. I removed the young birds which looked starkly different to adults, as well as some pictures of eggs that were in the data, and some drawings that were not photos . learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 0.085772 | 0.138853 | 0.043353 | 00:47 | . epoch train_loss valid_loss error_rate time . 0 | 0.070127 | 0.155185 | 0.034682 | 00:48 | . 1 | 0.078726 | 0.142527 | 0.043353 | 00:47 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(10, nrows=10) . Possible improvement notes: . I need to learn what these birds actually look like by definition so I can fix the training data better | I may be running fine tune too many times as it seems to be starting to overfit, but need to read up more on this | Learning from my hummingbird app experience I need to investigate if male and female of a species have distinguishing characteristics that require them to have separate categories to allow for better training, as well as looking at chicks/ducklings/young birds characteristics since these may also be very different from how an adult in the species looks | . Next steps . Go through the steps for exporting the model and publishing to Binder | .",
            "url": "https://redditech.github.io/reddi-hacking/2021/06/27/_06_23_Messing_around_with_fastai.html",
            "relUrl": "/2021/06/27/_06_23_Messing_around_with_fastai.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Using Fastai Tabular For Tabular Playground Series June 2021 Competition",
            "content": "Introduction . This is a modification of the &quot;first pass&quot; submission to the Homesite Competition on Kaggle Competition using Google Colab, but modifying some of the default parameters and maybe adding some learning from the initial exploratory data analysis on other projects to apply to this one, to see if I can applying what was learnt so far to see how it fairs in a submission. . Changes made: . Use TestTrainSplitter() for making test and validation sets more fairly weighted based on the bias of any input data towards negative results | Increase batch size to 1024 to make training shorter, but to still hopefully get a better predictor for it. Set a separate validator batch size to 128. | Increase the validation percentage to 0.25 | Fix the learning rate to 1e-3 | Increase epochs to 5 | Modified the cat_names and cont_names arrays with any initial EDA insights | Add a date part for dates | Add weight decay of 0.2 | . Setup fastai and Google drive . !pip install -Uqq fastai . from fastai.tabular.all import * . !pip install kaggle . Requirement already satisfied: kaggle in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (1.5.12) Requirement already satisfied: python-dateutil in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2.8.1) Requirement already satisfied: six&gt;=1.10 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (1.16.0) Requirement already satisfied: tqdm in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (4.61.1) Requirement already satisfied: urllib3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (1.26.4) Requirement already satisfied: python-slugify in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (5.0.2) Requirement already satisfied: certifi in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2021.5.30) Requirement already satisfied: requests in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from kaggle) (2.25.1) Requirement already satisfied: text-unidecode&gt;=1.3 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;kaggle) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /home/nissan/miniconda3/envs/fastai/lib/python3.9/site-packages (from requests-&gt;kaggle) (2.10) . Useful links here: . Documentation on Path library | Documentation on fastai extensions to Path library | . Path.cwd() . Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks&#39;) . Environment Setup . Setup the environment variables here (these will change as we play around to get the best model for submission) . Set the random seed so that the results are reproducible | Set the batch size for training | Set the batch size for validation | Set what portion of training data to set aside for validation | Set the number of epochs of training to run to not overfit | Set the learning rate to be used | Set the weight decay | . set_seed(42) bs = 1024 val_bs = 128 test_size = 0.25 epochs = 5 lr = 1e-3 wd=0.2 . Setup kaggle environment parameters . from kaggle import api . path = Path.cwd() path.ls() . (#10) [Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/.gitignore&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/.ipynb_checkpoints&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/2021-06-21-Getting-Data-In-Kaggle.ipynb&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/2021-06-27-Using-Fastai-For-tabular-playground-series-jun-2021.ipynb&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/2021_06_23_Messing_around_with_fastai.ipynb&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/ghtop_images&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/my_icons&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/README.md&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/Untitled.ipynb&#39;),Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/_data&#39;)] . This bit is to make sure I don&#39;t checkin my data to Github when I&#39;m finished . !touch .gitignore . !echo &quot;_data&quot; &gt; .gitignore . !head .gitignore . _data . !mkdir _data . mkdir: cannot create directory ‘_data’: File exists . os.chdir(&#39;_data&#39;) Path.cwd() . Path(&#39;/mnt/d/Code/GitHub/reddi-hacking/_notebooks/_data&#39;) . path = Path.cwd()/&quot;playground_Jun_2021_data&quot; path.mkdir(exist_ok=True) Path.BASE_PATH = path api.competition_download_cli(&#39;tabular-playground-series-jun-2021&#39;, path=path) . tabular-playground-series-jun-2021.zip: Skipping, found more recently modified local copy (use --force to force download) . path.ls() . (#5) [Path(&#39;sample_submission.csv&#39;),Path(&#39;submission8.csv&#39;),Path(&#39;tabular-playground-series-jun-2021.zip&#39;),Path(&#39;test.csv&#39;),Path(&#39;train.csv&#39;)] . file_extract(path/&quot;tabular-playground-series-jun-2021.zip&quot;) . path.ls() . (#5) [Path(&#39;sample_submission.csv&#39;),Path(&#39;submission8.csv&#39;),Path(&#39;tabular-playground-series-jun-2021.zip&#39;),Path(&#39;test.csv&#39;),Path(&#39;train.csv&#39;)] . Exploring the Playground data . df_train = pd.read_csv(path/&quot;train.csv&quot;, low_memory=False) df_train.head() . id feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 ... feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 target . 0 0 | 0 | 0 | 6 | 1 | 0 | 0 | 0 | 0 | 7 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | Class_6 | . 1 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | Class_6 | . 2 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 3 | 0 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | Class_2 | . 3 3 | 0 | 0 | 7 | 0 | 1 | 5 | 2 | 2 | 0 | ... | 0 | 4 | 0 | 2 | 2 | 0 | 4 | 3 | 0 | Class_8 | . 4 4 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_2 | . 5 rows × 77 columns . df_train.shape . (200000, 77) . df_train.describe() . id feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 ... feature_65 feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 . count 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.00000 | ... | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.000000 | 200000.00000 | 200000.000000 | . mean 99999.500000 | 0.972710 | 1.168365 | 2.219325 | 2.296735 | 0.793530 | 1.431105 | 1.010695 | 0.673090 | 1.94398 | ... | 1.798040 | 0.508695 | 1.827300 | 0.910370 | 1.603585 | 1.219210 | 0.806895 | 1.282925 | 2.94021 | 0.632005 | . std 57735.171256 | 3.941836 | 3.993407 | 6.476570 | 7.551858 | 2.935785 | 5.162746 | 3.949231 | 2.234949 | 3.93133 | ... | 5.053014 | 1.867330 | 7.188924 | 3.835182 | 4.877679 | 4.826003 | 2.458741 | 4.261420 | 10.78465 | 3.925310 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | . 25% 49999.750000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | . 50% 99999.500000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.00000 | 0.000000 | . 75% 149999.250000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 2.00000 | ... | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 2.000000 | 1.000000 | 1.000000 | 1.000000 | 1.00000 | 0.000000 | . max 199999.000000 | 61.000000 | 51.000000 | 64.000000 | 70.000000 | 38.000000 | 76.000000 | 43.000000 | 30.000000 | 38.00000 | ... | 54.000000 | 24.000000 | 79.000000 | 55.000000 | 65.000000 | 67.000000 | 30.000000 | 61.000000 | 130.00000 | 52.000000 | . 8 rows × 76 columns . True in df_train.isna() . False . df_test = pd.read_csv(path/&quot;test.csv&quot;, low_memory=False) df_test.head() . id feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 ... feature_65 feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 . 0 200000 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 200001 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 3 | 1 | 3 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | . 2 200002 | 0 | 1 | 7 | 1 | 0 | 0 | 0 | 0 | 6 | ... | 3 | 0 | 0 | 0 | 0 | 3 | 0 | 2 | 0 | 0 | . 3 200003 | 0 | 0 | 0 | 4 | 3 | 1 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 4 | 0 | 0 | . 4 200004 | 0 | 0 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 76 columns . df_test.shape . (100000, 76) . True in df_test.isna() . False . y_column = df_train.columns.difference(df_test.columns) . y_column . Index([&#39;target&#39;], dtype=&#39;object&#39;) . From this it looks like target is the value we want to predict. Let&#39;s take a look at this . type(df_train.target) . pandas.core.series.Series . df_train.target.unique() . array([&#39;Class_6&#39;, &#39;Class_2&#39;, &#39;Class_8&#39;, &#39;Class_3&#39;, &#39;Class_1&#39;, &#39;Class_5&#39;, &#39;Class_7&#39;, &#39;Class_9&#39;, &#39;Class_4&#39;], dtype=object) . type(df_train.target.unique()[0]) . str . df_train.target.isna().sum() . 0 . Set the y column type . Make this a category for the purpose of generating predictions as a classification . df_train.target = df_train.target.astype(dtype=&#39;category&#39;) . target_categories = df_train[&#39;target&#39;].unique() target_categories . [&#39;Class_6&#39;, &#39;Class_2&#39;, &#39;Class_8&#39;, &#39;Class_3&#39;, &#39;Class_1&#39;, &#39;Class_5&#39;, &#39;Class_7&#39;, &#39;Class_9&#39;, &#39;Class_4&#39;] Categories (9, object): [&#39;Class_6&#39;, &#39;Class_2&#39;, &#39;Class_8&#39;, &#39;Class_3&#39;, ..., &#39;Class_5&#39;, &#39;Class_7&#39;, &#39;Class_9&#39;, &#39;Class_4&#39;] . df_train[&#39;target&#39;].cat.set_categories(target_categories, inplace=True) . Let&#39;s see how the training data outcomes are balanced . df_train.target.describe() . count 200000 unique 9 top Class_6 freq 51811 Name: target, dtype: object . train_data_balance = pd.DataFrame(df_train[&quot;target&quot;]).groupby(&quot;target&quot;) . train_data_balance[&quot;target&quot;].describe() . count unique top freq . target . Class_6 51811 | 1 | Class_6 | 51811 | . Class_2 24431 | 1 | Class_2 | 24431 | . Class_8 51763 | 1 | Class_8 | 51763 | . Class_3 14798 | 1 | Class_3 | 14798 | . Class_1 9118 | 1 | Class_1 | 9118 | . Class_5 3064 | 1 | Class_5 | 3064 | . Class_7 14769 | 1 | Class_7 | 14769 | . Class_9 25542 | 1 | Class_9 | 25542 | . Class_4 4704 | 1 | Class_4 | 4704 | . It&#39;s not quite equally weighted, e.g. Class_4 and Class_5 are ten times less than Class_6 . First things first . id looks unique so we can test this and if true, make it the index | . Additionally, we should make sure to apply any changes to data types to both train and test data so predictions don&#39;t fail later on . df_train.id.value_counts(), df_test.id.value_counts() . (0 1 111331 1 49869 1 56014 1 53967 1 .. 183667 1 193908 1 195957 1 189814 1 2047 1 Name: id, Length: 200000, dtype: int64, 262144 1 269609 1 206158 1 212301 1 210252 1 .. 283295 1 285342 1 279197 1 281244 1 264191 1 Name: id, Length: 100000, dtype: int64) . df_train = df_train.set_index(&#39;id&#39;) df_test = df_test.set_index(&#39;id&#39;) . Goal: Better model training, refining fastai parameters, using EDA insights gathered to date . y_names = [y_column[0]] y_names . [&#39;target&#39;] . cont_names, cat_names = cont_cat_split(df_train, dep_var=y_names) len(cont_names), len(cat_names) . (72, 3) . EDA on the categorical and continuous feature splits . Functions to improve fastai&#39;s cont_cat_split choices . The goal here is to validate the splot, rearrange as needed, and explicitly set the dtype and categories for categorical columns. I am going to use what I wrote for another post with some functions that help speed up evaluating the splits and reassigning them better than the fastai defaults . First I&#39;ll create a triage list for any fields that can&#39;t be programmatically optimized. These are the ones we have to do manual steps for until I find a way to do better . triage = L() . Let&#39;s take a quick look at the descriptions of the categorical and continuous splits automatically done . df_train[cont_names].astype(&#39;object&#39;).describe() . feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 ... feature_65 feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 . count 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | ... | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | 200000 | . unique 42 | 37 | 48 | 59 | 30 | 55 | 40 | 26 | 28 | 51 | ... | 41 | 23 | 57 | 42 | 52 | 49 | 31 | 37 | 91 | 50 | . top 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . freq 146389 | 140375 | 122501 | 120767 | 154093 | 119020 | 150969 | 150350 | 117000 | 124466 | ... | 124517 | 165048 | 141642 | 145532 | 113213 | 132247 | 139966 | 123153 | 142555 | 169451 | . 4 rows × 72 columns . The first thing I notice, is that for a number of these fields, there is quite a low number of unique values. Luckily, I don&#39;t see any missing or NA values as all the count totals are equal to the total number of rows of data. These are a lot of fields to go through and manually recategorize and then get their categories, but I am hoping I can do this programmatically. . Let&#39;s have a look at the existing categoricals to make sure there&#39;s nothing suspeicious about these here either . df_train[cat_names].astype(&#39;object&#39;).describe() . feature_17 feature_26 feature_30 . count 200000 | 200000 | 200000 | . unique 15 | 20 | 18 | . top 0 | 0 | 0 | . freq 169538 | 127772 | 134843 | . Here I define two functions, which will help . to reset the cont_names and cat_names arrays with better fits of the actual data fields for those that are categorical, but were put into the continuous array. | For all cateogorical fields, it will also setup the categories for these fields and change their dtype | For any fields that have null values, it will remove them from their respective field, and place them in the triage list | . def reassign_to_categorical(field, df, continuous, categorical, triage): if df[field].isna().sum()==0: field_categories = df[field].unique() df[field] = df[field].astype(&#39;category&#39;) df[field].cat.set_categories(field_categories, inplace=True) if field in continuous: continuous.remove(field) if field not in categorical: categorical.append(field) else: if field in continuous: continuous.remove(field) if field in categorical: categorical.remove(field) triage.append(field) return df, continuous, categorical, triage . def categorize( df, cont_names, cat_names, triage, category_threshold): for field in df.columns: if ((len(df[field].unique()) &lt;= category_threshold) and (type(df[field].dtype) != pd.core.dtypes.dtypes.CategoricalDtype)): reassign_to_categorical(field, df, cont_names, cat_names, triage) return df, cont_names, cat_names, triage . df_train, cont_names, cat_names, triage = categorize(df_train, cont_names, cat_names, triage, 100) . len(cont_names), len(cat_names) . (1, 74) . So this is a big rebalancing between continuous and categorical fields. I saved alot of time with this function rather than doing this manually like I had for my initial data exploration. Let&#39;s take a look at how many came up for triaging still though . triage . (#0) [] . This was expected, since my triage fields would only be those that have NA values,so that we can look closer at the data to evaluate a FillMissing strategy, or if we have too many missing data values, to ignore the field in modelling . Prep the learner . Check to make sure I didn&#39;t make any typos, if the counts show any missing rows that&#39;s a sign . df_train.describe(include=&#39;all&#39;) . feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 ... feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 target . count 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | ... | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000.0 | 200000 | . unique 42.0 | 37.0 | 48.0 | 59.0 | 30.0 | 55.0 | 40.0 | 26.0 | 28.0 | 51.0 | ... | 23.0 | 57.0 | 42.0 | 52.0 | 49.0 | 31.0 | 37.0 | 91.0 | 50.0 | 9 | . top 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | Class_6 | . freq 146389.0 | 140375.0 | 122501.0 | 120767.0 | 154093.0 | 119020.0 | 150969.0 | 150350.0 | 117000.0 | 124466.0 | ... | 165048.0 | 141642.0 | 145532.0 | 113213.0 | 132247.0 | 139966.0 | 123153.0 | 142555.0 | 169451.0 | 51811 | . mean NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . std NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . min NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 25% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 50% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 75% NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . max NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 11 rows × 76 columns . True in df_train.isna() . False . &quot;id&quot; in cont_names, &quot;id&quot; in cat_names #Make sure we&#39;ve gotten our y-column excluded . (False, False) . procs = [Categorify, FillMissing, Normalize] splits = TrainTestSplitter(test_size=test_size, stratify=df_train[y_names])(df_train) . to = TabularPandas(df=df_train, procs=procs, cat_names=cat_names, cont_names=cont_names, y_names=y_names,splits=splits) dls = to.dataloaders(bs=bs, val_bs=val_bs, layers=[500,1000], ps=[0.01,0.001]) dls.valid.show_batch() . feature_17 feature_26 feature_30 feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 feature_10 feature_11 feature_12 feature_13 feature_14 feature_16 feature_18 feature_19 feature_20 feature_21 feature_22 feature_23 feature_24 feature_25 feature_27 feature_28 feature_29 feature_31 feature_32 feature_33 feature_34 feature_35 feature_36 feature_37 feature_38 feature_39 feature_40 feature_41 feature_42 feature_43 feature_44 feature_45 feature_46 feature_47 feature_48 feature_49 feature_50 feature_51 feature_52 feature_53 feature_54 feature_55 feature_56 feature_57 feature_58 feature_59 feature_60 feature_61 feature_62 feature_63 feature_64 feature_65 feature_66 feature_67 feature_68 feature_69 feature_70 feature_71 feature_72 feature_73 feature_74 feature_15 target . 0 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.692236e-08 | Class_2 | . 1 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 3 | 0 | 4 | 3 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 2 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 2 | 0 | 0 | 0 | 1 | 3 | 0 | 1 | 2 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 0 | 6 | 0 | 0 | 0 | 1 | 5 | 0 | 0 | 0 | 0 | 4 | 0 | 1 | 4.692236e-08 | Class_2 | . 2 6 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 1 | 0 | 0 | 1 | 2 | 1 | 0 | 0 | 1 | 0 | 0 | 8 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 4 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 2 | 1 | 0 | 0 | 8 | 1 | 4 | 0 | 0 | 0 | 2 | 0 | 1 | 0 | 0 | 15 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 4.692236e-08 | Class_6 | . 3 0 | 0 | 0 | 0 | 3 | 0 | 1 | 3 | 0 | 0 | 0 | 9 | 1 | 0 | 32 | 1 | 1 | 0 | 1 | 5 | 0 | 0 | 0 | 0 | 0 | 0 | 8 | 0 | 2 | 0 | 3 | 1 | 0 | 0 | 0 | 0 | 5 | 0 | 0 | 1 | 0 | 0 | 9 | 2 | 0 | 0 | 0 | 4 | 0 | 2 | 0 | 0 | 2 | 14 | 0 | 0 | 0 | 2 | 5 | 1 | 1 | 0 | 0 | 0 | 5 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 4.692236e-08 | Class_6 | . 4 2 | 0 | 0 | 1 | 23 | 0 | 1 | 0 | 1 | 1 | 0 | 0 | 41 | 0 | 1 | 0 | 1 | 1 | 0 | 1 | 2 | 3 | 4 | 0 | 0 | 0 | 14 | 2 | 0 | 12 | 4 | 54 | 2 | 1 | 0 | 0 | 1 | 11 | 3 | 13 | 0 | 0 | 3 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 42 | 0 | 3 | 54 | 0 | 25 | 0 | 0 | 0 | 1 | 0 | 3 | 2 | 19 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 4.692236e-08 | Class_2 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 4.692236e-08 | Class_4 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | 1 | 0 | 2 | 1 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 10 | 0 | 1 | 0 | 1 | 2 | 0 | 0 | 0 | 1 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 2 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 4.692236e-08 | Class_8 | . 7 0 | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 3 | 0 | 0 | 0 | 1 | 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 8 | 0 | 1 | 0 | 3 | 0 | 2 | 0 | 0 | 0 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 4 | 4 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 0 | 0 | 7 | 0 | 0 | 1 | 4 | 0 | 0 | 0 | 0 | 0 | 4.692236e-08 | Class_8 | . 8 0 | 0 | 0 | 6 | 1 | 0 | 1 | 1 | 2 | 0 | 5 | 1 | 0 | 0 | 0 | 2 | 0 | 3 | 2 | 6 | 0 | 5 | 0 | 0 | 1 | 2 | 3 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 2 | 2 | 2 | 6 | 4 | 1 | 3 | 0 | 3 | 4 | 1 | 7 | 0 | 1 | 0 | 2 | 1 | 0 | 4 | 18 | 0 | 2 | 0 | 1 | 1 | 1 | 0 | 4 | 1 | 0 | 7 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | 6 | 0 | 2.000000e+00 | Class_6 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4.692236e-08 | Class_9 | . len(dls.train)*bs, len(dls.valid)*val_bs . (149504, 50048) . learn = tabular_learner(dls, metrics=accuracy) . learn.lr_find() . SuggestedLRs(valley=tensor(0.0014)) . Reference to why we use fit_one_cycle . lr = 0.0014 epochs = 4 . learn.fit_one_cycle(epochs,lr, wd=wd) . epoch train_loss valid_loss accuracy time . 0 | 1.896143 | 1.822293 | 0.354460 | 00:08 | . 1 | 1.757242 | 1.751968 | 0.357080 | 00:07 | . 2 | 1.730326 | 1.753791 | 0.358260 | 00:07 | . 3 | 1.686104 | 1.768173 | 0.353260 | 00:08 | . preds, targs = learn.get_preds() . preds.shape . torch.Size([50000, 9]) . preds[0:1] . tensor([[0.2783, 0.1893, 0.1261, 0.1215, 0.0377, 0.0127, 0.0480, 0.1469, 0.0395]]) . len(preds) . 50000 . Doing inferences based on this blog post from Walk With Fastai initially, but then experimenting to get this . dl_test = dls.test_dl(df_test) . preds, _ = learn.get_preds(dl=dl_test) . preds.shape . torch.Size([100000, 9]) . Submission To Kaggle . path.ls() . (#6) [Path(&#39;sample_submission.csv&#39;),Path(&#39;submission8.csv&#39;),Path(&#39;submission9.csv&#39;),Path(&#39;tabular-playground-series-jun-2021.zip&#39;),Path(&#39;test.csv&#39;),Path(&#39;train.csv&#39;)] . df_submission = pd.read_csv(path/&quot;sample_submission.csv&quot;) #I could add `low_memory=false` but it makes things slower df_submission.head() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 200000 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 1 200001 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 2 200002 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 3 200003 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 4 200004 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . df_submission.tail() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 99995 299995 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 99996 299996 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 99997 299997 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 99998 299998 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . 99999 299999 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | 0.1111 | . len(df_test.index), len(preds) . (100000, 100000) . type(preds) . torch.Tensor . preds.dtype . torch.float32 . preds_for_submission = preds.tolist() preds_for_submission[0:1] . [[0.14556929469108582, 0.38398420810699463, 0.0492408387362957, 0.17785416543483734, 0.05465058609843254, 0.019792871549725533, 0.024652695283293724, 0.11572658270597458, 0.028528723865747452]] . submission = pd.DataFrame({&#39;id&#39;: df_test.index, &#39;Predictions&#39;: preds.tolist()}, columns=[&#39;id&#39;, &#39;Predictions&#39;]) . Needed to figure out how to extract the floating point value alone from the list to properly compose the csv output dataframe . type(submission.Predictions) . pandas.core.series.Series . type(submission.Predictions[0][0]) . float . submission.Predictions[0][0] . 0.14556929469108582 . submission.Predictions.tolist()[0] . [0.14556929469108582, 0.38398420810699463, 0.0492408387362957, 0.17785416543483734, 0.05465058609843254, 0.019792871549725533, 0.024652695283293724, 0.11572658270597458, 0.028528723865747452] . submission.head() . id Predictions . 0 200000 | [0.14556929469108582, 0.38398420810699463, 0.0492408387362957, 0.17785416543483734, 0.05465058609843254, 0.019792871549725533, 0.024652695283293724, 0.11572658270597458, 0.028528723865747452] | . 1 200001 | [0.30568018555641174, 0.06748361885547638, 0.2821882963180542, 0.069472536444664, 0.05176167190074921, 0.011320285499095917, 0.08354373276233673, 0.10796741396188736, 0.020582210272550583] | . 2 200002 | [0.7500181794166565, 0.026834947988390923, 0.08841108530759811, 0.02269136719405651, 0.009151597507297993, 0.007910830900073051, 0.026987241581082344, 0.05289360508322716, 0.015101218596100807] | . 3 200003 | [0.2330009788274765, 0.10760820657014847, 0.23307044804096222, 0.07857377827167511, 0.05705545097589493, 0.02363131381571293, 0.06864532828330994, 0.1658260077238083, 0.03258843347430229] | . 4 200004 | [0.3362259268760681, 0.10346393287181854, 0.2263432741165161, 0.08009770512580872, 0.04384588450193405, 0.009048418141901493, 0.0585983507335186, 0.11896053701639175, 0.02341604232788086] | . submission.index . RangeIndex(start=0, stop=100000, step=1) . submission[[&#39;Class_1&#39;,&#39;Class_2&#39;, &#39;Class_3&#39;, &#39;Class_4&#39;, &#39;Class_5&#39;, &#39;Class_6&#39;, &#39;Class_7&#39;, &#39;Class_8&#39;, &#39;Class_9&#39;]] = pd.DataFrame(submission.Predictions.tolist(), index= submission.index) . submission.head() . id Predictions Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 200000 | [0.14556929469108582, 0.38398420810699463, 0.0492408387362957, 0.17785416543483734, 0.05465058609843254, 0.019792871549725533, 0.024652695283293724, 0.11572658270597458, 0.028528723865747452] | 0.145569 | 0.383984 | 0.049241 | 0.177854 | 0.054651 | 0.019793 | 0.024653 | 0.115727 | 0.028529 | . 1 200001 | [0.30568018555641174, 0.06748361885547638, 0.2821882963180542, 0.069472536444664, 0.05176167190074921, 0.011320285499095917, 0.08354373276233673, 0.10796741396188736, 0.020582210272550583] | 0.305680 | 0.067484 | 0.282188 | 0.069473 | 0.051762 | 0.011320 | 0.083544 | 0.107967 | 0.020582 | . 2 200002 | [0.7500181794166565, 0.026834947988390923, 0.08841108530759811, 0.02269136719405651, 0.009151597507297993, 0.007910830900073051, 0.026987241581082344, 0.05289360508322716, 0.015101218596100807] | 0.750018 | 0.026835 | 0.088411 | 0.022691 | 0.009152 | 0.007911 | 0.026987 | 0.052894 | 0.015101 | . 3 200003 | [0.2330009788274765, 0.10760820657014847, 0.23307044804096222, 0.07857377827167511, 0.05705545097589493, 0.02363131381571293, 0.06864532828330994, 0.1658260077238083, 0.03258843347430229] | 0.233001 | 0.107608 | 0.233070 | 0.078574 | 0.057055 | 0.023631 | 0.068645 | 0.165826 | 0.032588 | . 4 200004 | [0.3362259268760681, 0.10346393287181854, 0.2263432741165161, 0.08009770512580872, 0.04384588450193405, 0.009048418141901493, 0.0585983507335186, 0.11896053701639175, 0.02341604232788086] | 0.336226 | 0.103464 | 0.226343 | 0.080098 | 0.043846 | 0.009048 | 0.058598 | 0.118961 | 0.023416 | . submission.drop([&quot;Predictions&quot;],axis=1, inplace=True) . submission.head() . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 200000 | 0.145569 | 0.383984 | 0.049241 | 0.177854 | 0.054651 | 0.019793 | 0.024653 | 0.115727 | 0.028529 | . 1 200001 | 0.305680 | 0.067484 | 0.282188 | 0.069473 | 0.051762 | 0.011320 | 0.083544 | 0.107967 | 0.020582 | . 2 200002 | 0.750018 | 0.026835 | 0.088411 | 0.022691 | 0.009152 | 0.007911 | 0.026987 | 0.052894 | 0.015101 | . 3 200003 | 0.233001 | 0.107608 | 0.233070 | 0.078574 | 0.057055 | 0.023631 | 0.068645 | 0.165826 | 0.032588 | . 4 200004 | 0.336226 | 0.103464 | 0.226343 | 0.080098 | 0.043846 | 0.009048 | 0.058598 | 0.118961 | 0.023416 | . submission.to_csv(path/&#39;submission10.csv&#39;, index=False) . api.competition_submit(path/&#39;submission10.csv&#39;,message=&quot;Tenth pass&quot;, competition=&#39;tabular-playground-series-jun-2021&#39;) . 100%|██████████| 17.9M/17.9M [00:10&lt;00:00, 1.81MB/s] . Successfully submitted to Tabular Playground Series - Jun 2021 . learn.save(&#39;tabular-playground-series-jun-2021_fastai_nn9&#39;) . Path(&#39;models/tabular-playground-series-jun-2021_fastai_nn9.pth&#39;) .",
            "url": "https://redditech.github.io/reddi-hacking/kaggle/fastai/2021/06/27/Using-Fastai-For-tabular-playground-series-jun-2021.html",
            "relUrl": "/kaggle/fastai/2021/06/27/Using-Fastai-For-tabular-playground-series-jun-2021.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "",
            "url": "https://redditech.github.io/reddi-hacking/2021/06/27/Untitled.html",
            "relUrl": "/2021/06/27/Untitled.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Getting Data From Kaggle",
            "content": "Introduction . Here I will play around with the Kaggle CLI to find an interesting competition, and load the data from it . This is a riff off my learning team blog post about getting specific data from Kaggle. You should reference it to understand the setups bit below for the why behind the instructions for setting up Kaggle-CLI and Fastai. Good links to reference: . Kaggle API documentation | . !pip install kaggle . Requirement already satisfied: kaggle in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.8.1) Requirement already satisfied: six&gt;=1.10 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.16.0) Requirement already satisfied: python-slugify in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (5.0.2) Requirement already satisfied: requests in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.25.1) Requirement already satisfied: urllib3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.26.4) Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (4.59.0) Requirement already satisfied: certifi in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2021.5.30) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (2.10) . !mamba install -c fastchan fastai -y . __ __ __ __ / / / / / / / / ███████████████/ /██/ /██/ /██/ /████████████████████████ / / / / / ____ / / _/ _/ _/ o __, / _/ _____/ ` |/ ███╗ ███╗ █████╗ ███╗ ███╗██████╗ █████╗ ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗ ██╔████╔██║███████║██╔████╔██║██████╔╝███████║ ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║ ██║ ╚═╝ ██║██║ ██║██║ ╚═╝ ██║██████╔╝██║ ██║ ╚═╝ ╚═╝╚═╝ ╚═╝╚═╝ ╚═╝╚═════╝ ╚═╝ ╚═╝ mamba (0.13.0) supported by @QuantStack GitHub: https://github.com/mamba-org/mamba Twitter: https://twitter.com/QuantStack █████████████████████████████████████████████████████████████ Looking for: [&#39;fastai&#39;] pkgs/r/osx-64 [&gt; ] (--:--) No change pkgs/r/osx-64 [====================] (00m:00s) No change pkgs/main/osx-64 [=&gt; ] (--:--) No change pkgs/main/osx-64 [====================] (00m:00s) No change pkgs/main/noarch [=&gt; ] (--:--) No change pkgs/main/noarch [====================] (00m:00s) No change pkgs/r/noarch [&gt; ] (--:--) No change pkgs/r/noarch [====================] (00m:00s) No change fastchan/osx-64 [=&gt; ] (--:--) No change fastchan/osx-64 [====================] (00m:00s) No change fastchan/noarch [=&gt; ] (--:--) No change fastchan/noarch [====================] (00m:00s) No change Transaction Prefix: /usr/local/anaconda3/envs/fastai All requested packages already installed . Let&#39;s say I want to find some getting started competitions, Kaggle CLI let&#39;s you do keyword searches . !kaggle competitions list -s &quot;Getting Started&quot; . ref deadline category reward teamCount userHasEntered -- - -- tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 936 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 3171 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 321 False acm-sf-chapter-hackathon-small 2012-09-30 01:00:00 Research $600 96 False getting-started 2012-02-26 00:00:00 Featured $10,000 0 False street-view-getting-started-with-julia 2017-01-07 00:00:00 Getting Started Knowledge 56 False . I know there&#39;s a Titanic dataset, but it isn&#39;t found with keyword searching, so let&#39;s try the category . !kaggle competitions list --category &quot;Getting Started&quot; . Invalid category specified. Valid options are [&#39;all&#39;, &#39;featured&#39;, &#39;research&#39;, &#39;recruitment&#39;, &#39;gettingStarted&#39;, &#39;masters&#39;, &#39;playground&#39;] . !kaggle competitions list --category gettingStarted . ref deadline category reward teamCount userHasEntered - - -- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 188 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 321 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 936 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 5879 True titanic 2030-01-01 00:00:00 Getting Started Knowledge 48369 False house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 12623 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 951 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 3171 False facial-keypoints-detection 2017-01-07 00:00:00 Getting Started Knowledge 175 False street-view-getting-started-with-julia 2017-01-07 00:00:00 Getting Started Knowledge 56 False word2vec-nlp-tutorial 2015-06-30 23:59:00 Getting Started Knowledge 577 False data-science-london-scikit-learn 2014-12-31 23:59:00 Getting Started Knowledge 190 False just-the-basics-the-after-party 2013-03-01 01:00:00 Getting Started Knowledge 48 False just-the-basics-strata-2013 2013-02-26 20:30:00 Getting Started Knowledge 49 False . There it is! This is where those fastai helper functions come in handy. I&#39;m going to work with tabular data, so I&#39;ll get this from the tabular library . from fastai.tabular.all import * . Path.cwd() . Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks&#39;) . Now I need to make sure I put this data in the correct place so it doesn&#39;t get checked in when I commit my changes in Github. I want to create a folder _data and have add an entry to my .gitignore to avoid the dataset I download into it from being checked in . !touch .gitignore . !echo &quot;_data&quot; &gt; .gitignore . !head .gitignore . _data . !mkdir _data . os.chdir(&#39;_data&#39;) Path.cwd() . Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data&#39;) . Ok, looks like I&#39;m ready to download that Titanic dataset . !kaggle competitions download -c titanic . 403 - Forbidden . Ok, this was a head scratcher, but it looks like before I can download any dataset, I need to go to the competition page and join the competition and accept the rules. I should have read the documentation that said this. Usual format for competition URLs is https://www.kaggle.com/c/&lt;competition-name&gt;/rules . !kaggle competitions download -c titanic . Downloading titanic.zip to /Users/nissan/code/reddi-hacking/_notebooks/_data 0%| | 0.00/34.1k [00:00&lt;?, ?B/s] 100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 1.41MB/s] . Now let&#39;s verify the file is there, and extract the data . path = Path.cwd() path.ls() . (#1) [Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/titanic.zip&#39;)] . file_extract(&#39;titanic.zip&#39;) . path.ls() . (#4) [Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/test.csv&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/titanic.zip&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/train.csv&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/gender_submission.csv&#39;)] . Using the Kaggle API . After I wrote above, I just read a chapter of the book that showed the Kaggle API is available programmatically, so I can use this instead of command line to load the data, and since I already have the tabular libraries loaded and load and take a first look at the top rows of the data . from kaggle import api . api.competitions_list(category=&#39;gettingStarted&#39;) . [contradictory-my-dear-watson, gan-getting-started, tpu-getting-started, digit-recognizer, titanic, house-prices-advanced-regression-techniques, connectx, nlp-getting-started, facial-keypoints-detection, street-view-getting-started-with-julia, word2vec-nlp-tutorial, data-science-london-scikit-learn, just-the-basics-the-after-party, just-the-basics-strata-2013] . api.competition_download_cli(&#39;titanic&#39;, path=path) file_extract(&quot;titanic.zip&quot;) . titanic.zip: Skipping, found more recently modified local copy (use --force to force download) . df = pd.read_csv(path/&#39;train.csv&#39;, skipinitialspace=True) df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | .",
            "url": "https://redditech.github.io/reddi-hacking/kaggle/2021/06/21/Getting-Data-In-Kaggle.html",
            "relUrl": "/kaggle/2021/06/21/Getting-Data-In-Kaggle.html",
            "date": " • Jun 21, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://redditech.github.io/reddi-hacking/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://redditech.github.io/reddi-hacking/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}