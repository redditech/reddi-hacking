{
  
    
        "post0": {
            "title": "Messing around with fastai - based on the fastbook content and fastai forum notes",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . Does some magic to setup my environment, here I&#39;m looking to see how hard it was to do this manually. The first thing I&#39;ll do is load up fastai instead of fastbook so I still have all the fastai libraries at hand if I need any core or helper functions . !rm -rf /content/sample_data/birds #cleanup for new run . !pip install -Uqq fastai . from fastai.vision.all import * . Running fastbook.setup_book?? showed that there was a boolean IN_COLLAB that called another function fastbook.setup_colab() to do the setup. Inside setup_collab it sets a global variable gdrive to my current path, and then imports google.collab to use drive to mount my google drive. I can do this myself here. . The snippets below are mostly straight out the fastbook source code in Github . global gdrive gdrive = Path(&#39;/content/gdrive/My Drive&#39;) from google.colab import drive if not gdrive.exists(): drive.mount(str(gdrive.parent)) . path = Path.cwd() . path . Path(&#39;/content&#39;) . There is also a nice little function for pulling images from Duck Duck Go instead of having to sign up to Microsoft Azure to get a key to use the Bing API for image search . def search_images_ddg(term, max_images=200): &quot;Search for `term` with DuckDuckGo and return a unique urls of about `max_images` images&quot; assert max_images&lt;1000 url = &#39;https://duckduckgo.com/&#39; res = urlread(url,data={&#39;q&#39;:term}) searchObj = re.search(r&#39;vqd=([ d-]+) &amp;&#39;, res) assert searchObj requestUrl = url + &#39;i.js&#39; params = dict(l=&#39;us-en&#39;, o=&#39;json&#39;, q=term, vqd=searchObj.group(1), f=&#39;,,,&#39;, p=&#39;1&#39;, v7exp=&#39;a&#39;) urls,data = set(),{&#39;next&#39;:1} while len(urls)&lt;max_images and &#39;next&#39; in data: try: data = urljson(requestUrl,data=params) urls.update(L(data[&#39;results&#39;]).itemgot(&#39;image&#39;)) requestUrl = url + data[&#39;next&#39;] except (URLError,HTTPError): pass time.sleep(0.2) return L(urls) . This code has a reference to an L object from fastcore, which in documentation on L indicates the intention of it is to replace lists. I should dive deeper into this to understand more. . I originally wrote a Trinidad hummingbird classifier from a previous attempt at the fastai course, in honor my native country of Trinidad and Tobago. This time, since I&#39;m now in Australia, and celebrating my 4th year as a Queenslander living in Brisbane, I wanted to take a look at classifying Brisbane birds. There are actually alot of them, I&#39;ll pick just a few that I actually have remembered seeing so I can do the data cleansing step with a fair level of confidence. I added three types of ducks as well, just to make it a little tougher I think, although I&#39;m not sure if I know the differences myself so we will see how my cleansing activities fare. The names are based on this list. Let&#39;s start with the most famous one, the kookaburra. . kookaburra_bird_images = search_images_ddg(&quot;laughing kookaburra bird&quot;) kookaburra_bird_images[0] . &#39;http://www.glenchilton.com/wp-content/uploads/2014/09/laughing_kookaburra_Ian-Montgomery-birdway-com-au.jpg&#39; . type(kookaburra_bird_images) . fastcore.foundation.L . type(kookaburra_bird_images[0]) . str . There&#39;s a method attrgot() that is used to extract the file names from a column when doing this with Bing in Lesson 2 I read up the documentation on and got further clarification on fastai forums here. Duck Duck Go returns just strings, so there&#39;s no need to extract a column attribute here. . path.ls() . (#3) [Path(&#39;/content/.config&#39;),Path(&#39;/content/gdrive&#39;),Path(&#39;/content/sample_data&#39;)] . dest = path/&#39;sample_data/kookaburra_bird.jpg&#39; download_url(kookaburra_bird_images[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . brisbane_bird_types = &quot;kookaburra&quot;,&quot;magpie goose&quot;,&quot;australian white ibis&quot;, &quot;australian pelican&quot;, &quot;pacific black duck&quot;, &quot;plumed whistling duck&quot;, &quot;australian wood duck&quot; path = path/&#39;sample_data/birds&#39; path . Path(&#39;/content/sample_data/birds&#39;) . if not path.exists(): path.mkdir() path . Path(&#39;/content/sample_data/birds&#39;) . path.ls() . (#0) [] . It may take some time to run with the default of 200 images. . for o in brisbane_bird_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_ddg(f&#39;{o} bird&#39;) download_images(dest, urls=results) . Download of http://www.environment.nsw.gov.au/images/nature/white_ibisLg.jpg has failed after 5 retries Fix the download manually: $ mkdir -p /content/sample_data/birds/australian white ibis $ cd /content/sample_data/birds/australian white ibis $ wget -c http://www.environment.nsw.gov.au/images/nature/white_ibisLg.jpg $ tar xf white_ibisLg.jpg And re-run your code once the download is successful . Let&#39;s verify we got images as we expected . path.ls() . (#7) [Path(&#39;/content/sample_data/birds/kookaburra&#39;),Path(&#39;/content/sample_data/birds/magpie goose&#39;),Path(&#39;/content/sample_data/birds/australian pelican&#39;),Path(&#39;/content/sample_data/birds/australian white ibis&#39;),Path(&#39;/content/sample_data/birds/pacific black duck&#39;),Path(&#39;/content/sample_data/birds/plumed whistling duck&#39;),Path(&#39;/content/sample_data/birds/australian wood duck&#39;)] . !ls &quot;sample_data/birds&quot; . &#39;australian pelican&#39; kookaburra &#39;plumed whistling duck&#39; &#39;australian white ibis&#39; &#39;magpie goose&#39; &#39;australian wood duck&#39; &#39;pacific black duck&#39; . get_image_files?? . fns = get_image_files(path) fns . (#1735) [Path(&#39;/content/sample_data/birds/kookaburra/00000173.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000145.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000195.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000044.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000047.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000143.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000101.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000188.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000158.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000074.jpg&#39;)...] . len(fns) . 1735 . Check for the corrupt images . failed = verify_images(fns) failed . (#4) [Path(&#39;/content/sample_data/birds/kookaburra/00000034.jpg&#39;),Path(&#39;/content/sample_data/birds/kookaburra/00000217.jpg&#39;),Path(&#39;/content/sample_data/birds/australian pelican/00000240.jpg&#39;),Path(&#39;/content/sample_data/birds/australian white ibis/00000119.jpg&#39;)] . Remove corrupted images . failed.map(Path.unlink); . I&#39;m going to bring forward the steps to clean up the data here from the book before cleaning, since I&#39;m pretty sure some of these aren&#39;t going to be bird images so I want to get rid of them. Since we don&#39;t have a classifier yet, we&#39;re going to have to use ImagesCleaner to do this pre-training cleaning step . from fastai.vision.widgets import * . We need the widgets to do cleanup. Reference this forum post . We need to see how to browse the individual directories since I don&#39;t think ImagesCleaner gives that directory selector that the ImageClassifierCleaner module does, so let&#39;s peek at how that works . ImageClassifierCleaner?? . ImagesCleaner?? . path.cwd() . Path(&#39;/content&#39;) . path = Path(&#39;sample_data/birds&#39;) . path.ls() . (#7) [Path(&#39;sample_data/birds/kookaburra&#39;),Path(&#39;sample_data/birds/magpie goose&#39;),Path(&#39;sample_data/birds/australian pelican&#39;),Path(&#39;sample_data/birds/australian white ibis&#39;),Path(&#39;sample_data/birds/pacific black duck&#39;),Path(&#39;sample_data/birds/plumed whistling duck&#39;),Path(&#39;sample_data/birds/australian wood duck&#39;)] . Rerun the next three cells substituting the different categories of kookaburra with magpie goose,australian white ibis, australian pelican, pacific black duck, plumed whistling duck, australian wood duck to do some initial pre-training cleaning of possibly irrelevant images from search engine . fns_to_clean = get_image_files(path/&#39;kookaburra&#39;) fns_to_clean . (#234) [Path(&#39;sample_data/birds/kookaburra/00000173.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000145.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000195.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000044.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000047.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000143.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000101.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000188.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000158.jpg&#39;),Path(&#39;sample_data/birds/kookaburra/00000074.jpg&#39;)...] . cleaner = ImagesCleaner() cleaner.set_fns(fns_to_clean) cleaner . Now that we&#39;ve marked the ones that should get remove, let&#39;s take them out of the image data before we start training our model . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . path.ls() . (#7) [Path(&#39;sample_data/birds/kookaburra&#39;),Path(&#39;sample_data/birds/magpie goose&#39;),Path(&#39;sample_data/birds/australian pelican&#39;),Path(&#39;sample_data/birds/australian white ibis&#39;),Path(&#39;sample_data/birds/pacific black duck&#39;),Path(&#39;sample_data/birds/plumed whistling duck&#39;),Path(&#39;sample_data/birds/australian wood duck&#39;)] . Load the data into the datablock . birds = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = birds.dataloaders(path) . dls.valid.show_batch(max_n=10, nrows=2) . Squishing large images to fit into the size of the image . birds = birds.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = birds.dataloaders(path) dls.valid.show_batch(max_n=10, nrows=2) . Padding the image borders so they fit . birds = birds.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = birds.dataloaders(path) dls.valid.show_batch(max_n=10, nrows=2) . Randomly resizing images to allow it to learn on specific parts of an image during epoch . birds = birds.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = birds.dataloaders(path) dls.train.show_batch(max_n=10, nrows=2, unique=True) . Starting the data augmentation step here, which does a randomization of all three previous steps. . TODO:Add the individual keywords for image rotation, flipping, perspective warping, brightness changes and contrast changes. . birds = birds.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = birds.dataloaders(path) dls.train.show_batch(max_n=8, nrows=2, unique=True) . /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1023: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:760.) ret = func(*args, **kwargs) . Putting it all together now to train our model . birds = birds.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = birds.dataloaders(path) . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.692304 | 0.229488 | 0.078035 | 00:46 | . epoch train_loss valid_loss error_rate time . 0 | 0.362119 | 0.178386 | 0.072254 | 00:47 | . 1 | 0.267956 | 0.136552 | 0.046243 | 00:46 | . 2 | 0.220220 | 0.170266 | 0.049133 | 00:47 | . 3 | 0.163554 | 0.166847 | 0.049133 | 00:47 | . Take a look at how well it did . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Let&#39;s see where it went wrong . interp.plot_top_losses(10, nrows=10) . If I missed some categorization pre-training, now I can use the code from the book to fix these mistakes here . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . I can now re-run the learner with the fixed dataset and see if that helps improve anything. I removed the young birds which looked starkly different to adults, as well as some pictures of eggs that were in the data, and some drawings that were not photos . learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 0.085772 | 0.138853 | 0.043353 | 00:47 | . epoch train_loss valid_loss error_rate time . 0 | 0.070127 | 0.155185 | 0.034682 | 00:48 | . 1 | 0.078726 | 0.142527 | 0.043353 | 00:47 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(10, nrows=10) . Possible improvement notes: . I need to learn what these birds actually look like by definition so I can fix the training data better | I may be running fine tune too many times as it seems to be starting to overfit, but need to read up more on this | Learning from my hummingbird app experience I need to investigate if male and female of a species have distinguishing characteristics that require them to have separate categories to allow for better training, as well as looking at chicks/ducklings/young birds characteristics since these may also be very different from how an adult in the species looks | . Next steps . Go through the steps for exporting the model and publishing to Binder | .",
            "url": "https://redditech.github.io/reddi-hacking/2021/06/22/_06_23_Messing_around_with_fastai.html",
            "relUrl": "/2021/06/22/_06_23_Messing_around_with_fastai.html",
            "date": " • Jun 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting Data From Kaggle",
            "content": "Introduction . Here I will play around with the Kaggle CLI to find an interesting competition, and load the data from it . This is a riff off my learning team blog post about getting specific data from Kaggle. You should reference it to understand the setups bit below for the why behind the instructions for setting up Kaggle-CLI and Fastai. Good links to reference: . Kaggle API documentation | . !pip install kaggle . Requirement already satisfied: kaggle in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (1.5.12) Requirement already satisfied: python-dateutil in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.8.1) Requirement already satisfied: six&gt;=1.10 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.16.0) Requirement already satisfied: python-slugify in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (5.0.2) Requirement already satisfied: requests in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2.25.1) Requirement already satisfied: urllib3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (1.26.4) Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (4.59.0) Requirement already satisfied: certifi in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from kaggle) (2021.5.30) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (4.0.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/anaconda3/envs/fastai/lib/python3.8/site-packages (from requests-&gt;kaggle) (2.10) . !mamba install -c fastchan fastai -y . __ __ __ __ / / / / / / / / ███████████████/ /██/ /██/ /██/ /████████████████████████ / / / / / ____ / / _/ _/ _/ o __, / _/ _____/ ` |/ ███╗ ███╗ █████╗ ███╗ ███╗██████╗ █████╗ ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗ ██╔████╔██║███████║██╔████╔██║██████╔╝███████║ ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║ ██║ ╚═╝ ██║██║ ██║██║ ╚═╝ ██║██████╔╝██║ ██║ ╚═╝ ╚═╝╚═╝ ╚═╝╚═╝ ╚═╝╚═════╝ ╚═╝ ╚═╝ mamba (0.13.0) supported by @QuantStack GitHub: https://github.com/mamba-org/mamba Twitter: https://twitter.com/QuantStack █████████████████████████████████████████████████████████████ Looking for: [&#39;fastai&#39;] pkgs/r/osx-64 [&gt; ] (--:--) No change pkgs/r/osx-64 [====================] (00m:00s) No change pkgs/main/osx-64 [=&gt; ] (--:--) No change pkgs/main/osx-64 [====================] (00m:00s) No change pkgs/main/noarch [=&gt; ] (--:--) No change pkgs/main/noarch [====================] (00m:00s) No change pkgs/r/noarch [&gt; ] (--:--) No change pkgs/r/noarch [====================] (00m:00s) No change fastchan/osx-64 [=&gt; ] (--:--) No change fastchan/osx-64 [====================] (00m:00s) No change fastchan/noarch [=&gt; ] (--:--) No change fastchan/noarch [====================] (00m:00s) No change Transaction Prefix: /usr/local/anaconda3/envs/fastai All requested packages already installed . Let&#39;s say I want to find some getting started competitions, Kaggle CLI let&#39;s you do keyword searches . !kaggle competitions list -s &quot;Getting Started&quot; . ref deadline category reward teamCount userHasEntered -- - -- tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 936 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 3171 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 321 False acm-sf-chapter-hackathon-small 2012-09-30 01:00:00 Research $600 96 False getting-started 2012-02-26 00:00:00 Featured $10,000 0 False street-view-getting-started-with-julia 2017-01-07 00:00:00 Getting Started Knowledge 56 False . I know there&#39;s a Titanic dataset, but it isn&#39;t found with keyword searching, so let&#39;s try the category . !kaggle competitions list --category &quot;Getting Started&quot; . Invalid category specified. Valid options are [&#39;all&#39;, &#39;featured&#39;, &#39;research&#39;, &#39;recruitment&#39;, &#39;gettingStarted&#39;, &#39;masters&#39;, &#39;playground&#39;] . !kaggle competitions list --category gettingStarted . ref deadline category reward teamCount userHasEntered - - -- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 188 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 321 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 936 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 5879 True titanic 2030-01-01 00:00:00 Getting Started Knowledge 48369 False house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 12623 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 951 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 3171 False facial-keypoints-detection 2017-01-07 00:00:00 Getting Started Knowledge 175 False street-view-getting-started-with-julia 2017-01-07 00:00:00 Getting Started Knowledge 56 False word2vec-nlp-tutorial 2015-06-30 23:59:00 Getting Started Knowledge 577 False data-science-london-scikit-learn 2014-12-31 23:59:00 Getting Started Knowledge 190 False just-the-basics-the-after-party 2013-03-01 01:00:00 Getting Started Knowledge 48 False just-the-basics-strata-2013 2013-02-26 20:30:00 Getting Started Knowledge 49 False . There it is! This is where those fastai helper functions come in handy. I&#39;m going to work with tabular data, so I&#39;ll get this from the tabular library . from fastai.tabular.all import * . Path.cwd() . Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks&#39;) . Now I need to make sure I put this data in the correct place so it doesn&#39;t get checked in when I commit my changes in Github. I want to create a folder _data and have add an entry to my .gitignore to avoid the dataset I download into it from being checked in . !touch .gitignore . !echo &quot;_data&quot; &gt; .gitignore . !head .gitignore . _data . !mkdir _data . os.chdir(&#39;_data&#39;) Path.cwd() . Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data&#39;) . Ok, looks like I&#39;m ready to download that Titanic dataset . !kaggle competitions download -c titanic . 403 - Forbidden . Ok, this was a head scratcher, but it looks like before I can download any dataset, I need to go to the competition page and join the competition and accept the rules. I should have read the documentation that said this. Usual format for competition URLs is https://www.kaggle.com/c/&lt;competition-name&gt;/rules . !kaggle competitions download -c titanic . Downloading titanic.zip to /Users/nissan/code/reddi-hacking/_notebooks/_data 0%| | 0.00/34.1k [00:00&lt;?, ?B/s] 100%|██████████████████████████████████████| 34.1k/34.1k [00:00&lt;00:00, 1.41MB/s] . Now let&#39;s verify the file is there, and extract the data . path = Path.cwd() path.ls() . (#1) [Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/titanic.zip&#39;)] . file_extract(&#39;titanic.zip&#39;) . path.ls() . (#4) [Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/test.csv&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/titanic.zip&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/train.csv&#39;),Path(&#39;/Users/nissan/code/reddi-hacking/_notebooks/_data/gender_submission.csv&#39;)] . Using the Kaggle API . After I wrote above, I just read a chapter of the book that showed the Kaggle API is available programmatically, so I can use this instead of command line to load the data, and since I already have the tabular libraries loaded and load and take a first look at the top rows of the data . from kaggle import api . api.competitions_list(category=&#39;gettingStarted&#39;) . [contradictory-my-dear-watson, gan-getting-started, tpu-getting-started, digit-recognizer, titanic, house-prices-advanced-regression-techniques, connectx, nlp-getting-started, facial-keypoints-detection, street-view-getting-started-with-julia, word2vec-nlp-tutorial, data-science-london-scikit-learn, just-the-basics-the-after-party, just-the-basics-strata-2013] . api.competition_download_cli(&#39;titanic&#39;, path=path) file_extract(&quot;titanic.zip&quot;) . titanic.zip: Skipping, found more recently modified local copy (use --force to force download) . df = pd.read_csv(path/&#39;train.csv&#39;, skipinitialspace=True) df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | .",
            "url": "https://redditech.github.io/reddi-hacking/kaggle/2021/06/21/Getting-Data-In-Kaggle.html",
            "relUrl": "/kaggle/2021/06/21/Getting-Data-In-Kaggle.html",
            "date": " • Jun 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://redditech.github.io/reddi-hacking/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://redditech.github.io/reddi-hacking/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://redditech.github.io/reddi-hacking/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}